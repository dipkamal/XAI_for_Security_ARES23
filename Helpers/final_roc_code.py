# -*- coding: utf-8 -*-
"""Final_drop_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jTuzdP0xJdiqqD8ehu543XAfyNMbqH_C
"""

!pip install captum

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import torch.utils.data as utils
import torch.utils.data as td
torch.manual_seed(1234)

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder

import tensorflow as tf

# imports from captum library
from captum.attr import (
    IntegratedGradients,
    DeepLift,
    GradientShap,
    NoiseTunnel,
    Saliency,
    InputXGradient,
    Occlusion,
)
from captum.attr import DeepLiftShap, KernelShap, LimeBase
import matplotlib.pyplot as plt
from captum._utils.models.linear_model import SkLearnLinearModel

# Load datasets
path_to_csv = "contagio-all.csv"
df = pd.read_csv(path_to_csv)
df =df.drop(['filename'], axis=1)

# Pandas dataframes to numpy arrays
X = df.drop(["class"], axis=1).values
Y = df["class"].values

from sklearn.preprocessing import normalize

binary_encoding = False 
if binary_encoding: 
    X[np.where(data!= 0)] = 1
else:
    X = normalize(X, 'max', axis=0)

train_features, test_features, train_labels, test_labels = train_test_split(
  X, Y, test_size=0.3,random_state=42)

class NNModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(135, 135)
        self.sigmoid1 = nn.Sigmoid()
        self.linear2 = nn.Linear(135, 100)
        self.sigmoid2 = nn.Sigmoid()
        self.linear3 = nn.Linear(100, 8)
        self.sigmoid3 = nn.Sigmoid()
        self.linear4 = nn.Linear(8, 2)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        lin1_out = self.linear1(x)
        sigmoid_out1 = self.sigmoid1(lin1_out)
        sigmoid_out2 = self.sigmoid2(self.linear2(sigmoid_out1))
        sigmoid_out3 = self.sigmoid3(self.linear3(sigmoid_out2))
        return self.softmax(self.linear4(sigmoid_out3))
        # return self.linear4(sigmoid_out3)

net = NNModel()

criterion = nn.CrossEntropyLoss()
num_epochs = 300

optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
input_tensor = torch.from_numpy(train_features).type(torch.FloatTensor)
label_tensor = torch.from_numpy(train_labels)


for epoch in range(num_epochs):
    output = net(input_tensor)
    loss = criterion(output, label_tensor)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if epoch % 20 == 0:
        print("Epoch {}/{} => Loss: {:.2f}".format(epoch + 1, num_epochs, loss.item()))

path = "PDFClassifierFinal.pth"
net = NNModel()
net.load_state_dict(torch.load(path))
net.eval()

path_to_csv = "contagio-all.csv"
data = pd.read_csv(path_to_csv, dtype=str, delimiter=',',)
data = data.drop(['filename'], axis=1)
data = data.drop(['class'], axis=1)
feature_names = data.columns
x_axis_data = np.arange(test_features.shape[1])

x_axis_data

x_axis_data_labels = list(map(lambda idx: feature_names[idx], x_axis_data))


x_axis_data_labels

def similarity_kernel(original_input, perturbed_input, perturbed_interpretable_input,**kwargs):
    kernel_width = kwargs["kernel_width"]
    l2_dist = torch.norm(original_input - perturbed_input)
    return torch.exp(- (l2_dist**2) / (kernel_width**2))

def perturb_func(original_input,**kwargs):
    return original_input + torch.randn_like(original_input)

def to_interp_transform(curr_sample, original_inp,**kwargs):
    return curr_sample



def safe_main_func(exp_method):

  tp_ip = [exp_method]

  sa = Saliency(net)
  ks = KernelShap(net)
  ig = IntegratedGradients(net)
  dl = DeepLift(net)
  gs = GradientShap(net)
  sa = Saliency(net)
  ixg = InputXGradient(net)
  occ = Occlusion(net)
  dls = DeepLiftShap(net)


  test_input_tensor = torch.from_numpy(test_features).type(torch.FloatTensor)
  test_labels_tensor =  torch.from_numpy(test_labels)


  test_feature = test_input_tensor[0:600]
  test_label = test_labels_tensor[0:600]



  # test_feature = test_input_tensor[:]
  # test_label = test_labels_tensor[:]

  new_test_label = []
  new_test_feature = []
  for i in range(len(test_label)):
      if (test_label[i] == 0 ):
          new_test_label.append(test_label[i])
          new_test_feature.append(test_feature[i])

  # print(len(new_test_feature))

  new_test_feature_tensor = torch.stack(new_test_feature)
  new_test_label_tensor = torch.stack(new_test_label)


  # print(new_test_feature_tensor[2].unsqueeze(0))

  # print(len(new_test_feature_tensor))

  la_la_list = []
  # for i in range(0,len(new_test_feature_tensor)):
# for i in [25]:
  # print("========entering next input ========")
  # print(new_test_feature_tensor[0:i].shape)


  # ip = new_test_feature_tensor[i]
  # op = new_test_label_tensor[i]

  # print(ip)
  # new_test_feature_tensor_temp = ip.unsqueeze(0)
  # # print(new_test_feature_tensor_temp.shape)
  # new_test_label_tensor_temp = op.unsqueeze(0)
  

  #for loop will come in place
  # methods = ["ig","dl","gs","sa","ixg","occ","dls"]
  # methods = ["ig","sa","dl","gs","ixg","occ","ks","dls"]
  # methods = ["occ","ixg","ig","dl"]
  methods = tp_ip

  for explainers in methods:

    if explainers == "Saliency":
      sa_attr_test = sa.attribute(new_test_feature_tensor, target = new_test_label_tensor)

      sa_attr_test_sum = sa_attr_test.detach().numpy().sum(0)
      sa_attr_test_norm_sum = sa_attr_test_sum / np.linalg.norm(sa_attr_test_sum, ord=1)


      #for simple gradient method
      #top_15_ind = np.argpartition(sa_attr_test_norm_sum, -15)[-15:]
      sa_top_50_ind = np.argsort(sa_attr_test_norm_sum)[:-10]
      sa_top_50_ind = np.flip(sa_top_50_ind)

      sa_bool_arr = (sa_attr_test_norm_sum <= 0)
      sa_indices = np.where(sa_bool_arr)[0]

      # print(top_15_ind)
      # print(sa_attr_test_norm_sum[top_15_ind])
      # new_array = sa_attr_test_norm_sum[top_15_ind]
      # # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # # top_15_labels.reverse()

      # print(top_15_labels)
    
    elif explainers == "Integrated Gradients":

      ig_attr_test = ig.attribute(new_test_feature_tensor, target = new_test_label_tensor, n_steps=50)
      ig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)
      ig_attr_test_norm_sum = ig_attr_test_sum / np.linalg.norm(ig_attr_test_sum, ord=1)

      ig_top_50_ind = np.argsort(ig_attr_test_norm_sum)[:-10]
      ig_top_50_ind = np.flip(ig_top_50_ind)

      ig_bool_arr = (ig_attr_test_norm_sum <= 0)
      ig_indices = np.where(ig_bool_arr)[0]

      # new_array = ig_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # top_15_labels
      # print(top_15_labels)

    # methods = ["Integrated Gradients","Saliency","Deep Lift","Gradient Shap","InputXGradient","Occlusion","Kernel Shap"]

    elif explainers == "DeepLift":

      dl_attr_test = dl.attribute(new_test_feature_tensor, target = new_test_label_tensor,)
      dl_attr_test_sum = dl_attr_test.detach().numpy().sum(0)
      # print(dl_attr_test_sum)
      dl_attr_test_norm_sum = dl_attr_test_sum / np.linalg.norm(dl_attr_test_sum, ord=1)
      # print(dl_attr_test_norm_sum)
      # print(type(dl_attr_test_norm_sum))

      dl_top_50_ind = np.argsort(dl_attr_test_norm_sum)[:-10]
      dl_top_50_ind = np.flip(dl_top_50_ind)

      dl_bool_arr = (dl_attr_test_norm_sum <= 0)
      dl_indices = np.where(dl_bool_arr)[0]

      # new_array = dl_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # top_15_labels
      # print(top_15_labels)

    elif explainers == "GradientShap":

      gs_attr_test = gs.attribute(new_test_feature_tensor, target = new_test_label_tensor, baselines = input_tensor[1:50])
      gs_attr_test_sum = gs_attr_test.detach().numpy().sum(0)
      gs_attr_test_norm_sum = gs_attr_test_sum / np.linalg.norm(gs_attr_test_sum, ord=1)
  
      gs_top_50_ind = np.argsort(gs_attr_test_norm_sum)[:-10]
      # print("before flipping")
      # print(top_15_ind)
      # print("after flipping")
      gs_top_50_ind = np.flip(gs_top_50_ind)
      # print(top_15_ind)

      gs_bool_arr = (gs_attr_test_norm_sum <= 0)
      gs_indices = np.where(gs_bool_arr)[0]

      # new_array = gs_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # top_15_labels
      # # print(top_15_labels)
    
    elif explainers == "InputXGradient":
      
      inputXgradient_attr_test = ixg.attribute(new_test_feature_tensor, target = new_test_label_tensor,)
      inputxgradient_attr_test_sum = inputXgradient_attr_test.detach().numpy().sum(0)
      inputxgradient_attr_test_norm_sum = inputxgradient_attr_test_sum / np.linalg.norm(inputxgradient_attr_test_sum, ord=1)



      ixg_top_50_ind = np.argsort(inputxgradient_attr_test_norm_sum)[:-10]
      ixg_top_50_ind = np.flip(ixg_top_50_ind)


      ixg_bool_arr = (inputxgradient_attr_test_norm_sum <= 0)
      ixg_indices = np.where(ixg_bool_arr)[0]

      # new_array = inputxgradient_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # # top_15_labels

    elif explainers == "Occlusion":
      occl_attr_test = occ.attribute(new_test_feature_tensor, sliding_window_shapes=(3,))
      occl_attr_test_sum = occl_attr_test.detach().numpy().sum(0)
      occl_attr_test_norm_sum = occl_attr_test_sum / np.linalg.norm(occl_attr_test_sum, ord=1)


      occ_top_50_ind = np.argsort(occl_attr_test_norm_sum)[:-10]
      occ_top_50_ind = np.flip(occ_top_50_ind)

      occ_bool_arr = (occl_attr_test_norm_sum <= 0)
      occ_indices = np.where(occ_bool_arr)[0]

      # new_array = occl_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # # top_15_labels
    elif explainers == "Shap":

      ks_attr_test = ks.attribute(new_test_feature_tensor, target = new_test_label_tensor)

      ks_attr_test_sum = ks_attr_test.detach().numpy().sum(0)
      ks_attr_test_norm_sum = ks_attr_test_sum / np.linalg.norm(ks_attr_test_sum, ord=1)
    

      ks_top_50_ind = np.argsort(ks_attr_test_norm_sum)[:-10]
      ks_top_50_ind = np.flip(ks_top_50_ind)

      ks_bool_arr = (ks_attr_test_norm_sum <= 0)
      ks_indices = np.where(ks_bool_arr)[0]

      # new_array = ks_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # top_15_labels

    elif explainers == "Lime":
      
      # new_test_feature_tensor = new_test_feature_tensor_temp.squeeze()
      # print(new_test_feature_tensor.shape)
      # new_test_label_tensor = new_test_label_tensor_temp.squeeze()

      for i in range(0,len(new_test_feature_tensor)):

        ip = new_test_feature_tensor[i]
        op = new_test_label_tensor[i]

        # print(ip)
        new_test_feature_tensor_temp = ip.unsqueeze(0)
        # print(new_test_feature_tensor_temp.shape)
        new_test_label_tensor_temp = op.unsqueeze(0)


        lime_attr = LimeBase(net,
                       SkLearnLinearModel("linear_model.Ridge"),
                       similarity_func=similarity_kernel,
                       perturb_func=perturb_func,
                       perturb_interpretable_space=False,
                       from_interp_rep_transform=None,
                       to_interp_rep_transform=to_interp_transform)
      
        lime_attr = lime_attr.attribute(new_test_feature_tensor_temp, target = new_test_label_tensor_temp, kernel_width=1.1)
        lime_attr_test_sum = lime_attr.detach().numpy().sum(0)
        lime_attr_test_norm_sum = lime_attr_test_sum / np.linalg.norm(lime_attr_test_sum, ord=1)

        lime_top_50_ind = np.argsort(lime_attr_test_norm_sum)[:-10]
        lime_top_50_ind = np.flip(lime_top_50_ind)

        lime_bool_arr = (lime_attr_test_norm_sum <= 0)
        lime_indices = np.where(lime_bool_arr)[0]

        for idx in lime_top_50_ind:
      #   # print(new_test_feature_tensor_temp[0][idx])
          new_test_feature_tensor_temp[0][idx] = 0

        new_test_feature_tensor[i] = new_test_feature_tensor_temp

    # elif explainers == "dls":
    #   baselines = torch.zeros(new_test_feature_tensor.shape)
    #   dls_attr_test = dls.attribute(new_test_feature_tensor, target = new_test_label_tensor, baselines=baselines)



    #   dls_attr_test_sum = dls_attr_test.detach().numpy().sum(0)
    #   dls_attr_test_norm_sum = dls_attr_test_sum / np.linalg.norm(dls_attr_test_sum, ord=1)

    #   dls_top_50_ind = np.argsort(dls_attr_test_norm_sum)[-50:]
    #   dls_top_50_ind = np.flip(dls_top_50_ind)


      # new_array = dls_attr_test_norm_sum[dls_top_50_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # # top_15_labels



      # methods = ["ig","sa","dl","gs","ixg","occ","ks","dls"]


    # if explainers == "Integrated Gradients":
    #   top_15_ind = ig_indices
    # elif explainers == "Saliency":
    #   top_15_ind = sa_indices
    # elif explainers == "DeepLift":
    #   top_15_ind = dl_indices
    # elif explainers == "GradientShap":
    #   top_15_ind = gs_indices
    # elif explainers == "InputXGradient":
    #   top_15_ind = ixg_indices
    # elif explainers == "Occlusion":
    #   top_15_ind = occ_indices
    # elif explainers == "Shap":
    #   top_15_ind = ks_indices

    if explainers == "Integrated Gradients":
      top_15_ind = ig_top_50_ind
    elif explainers == "Saliency":
      top_15_ind = sa_top_50_ind
    elif explainers == "DeepLift":
      top_15_ind = dl_top_50_ind
    elif explainers == "GradientShap":
      top_15_ind = gs_top_50_ind
    elif explainers == "InputXGradient":
      top_15_ind = ixg_top_50_ind
    elif explainers == "Occlusion":
      top_15_ind = occ_top_50_ind
    elif explainers == "Shap":
      top_15_ind = ks_top_50_ind
    # elif explainers == "Lime":
    #   top_15_ind = lime_top_50_ind
    # elif explainers == "dls":
    #   top_15_ind = dls_top_50_ind


    # for idx in top_15_ind:
    #   # print(new_test_feature_tensor_temp[0][idx])
    #   new_test_feature_tensor_temp[0][idx] = 0

    # new_test_feature_tensor_temp[top_15_ind] = 0
    # new_test_feature_tensor[i] = new_test_feature_tensor_temp
    if explainers != "Lime":
      for i in range(0,len(new_test_feature_tensor)):
        
        for idx in top_15_ind:
        #   # print(new_test_feature_tensor_temp[0][idx])
          new_test_feature_tensor[i][idx] = 0


  return new_test_feature_tensor,new_test_label_tensor




def mal_main_func(exp_method):

  tp_ip = [exp_method]

  sa = Saliency(net)
  ks = KernelShap(net)
  ig = IntegratedGradients(net)
  dl = DeepLift(net)
  gs = GradientShap(net)
  sa = Saliency(net)
  ixg = InputXGradient(net)
  occ = Occlusion(net)
  dls = DeepLiftShap(net)


  test_input_tensor = torch.from_numpy(test_features).type(torch.FloatTensor)
  test_labels_tensor =  torch.from_numpy(test_labels)


  test_feature = test_input_tensor[0:600]
  test_label = test_labels_tensor[0:600]



  # test_feature = test_input_tensor[:]
  # test_label = test_labels_tensor[:]

  new_test_label = []
  new_test_feature = []
  for i in range(len(test_label)):
      if (test_label[i] == 1 ):
          new_test_label.append(test_label[i])
          new_test_feature.append(test_feature[i])

  # print(len(new_test_feature))

  new_test_feature_tensor = torch.stack(new_test_feature)
  new_test_label_tensor = torch.stack(new_test_label)


  # print(new_test_feature_tensor[2].unsqueeze(0))

  # print(len(new_test_feature_tensor))

  la_la_list = []
  # for i in range(0,len(new_test_feature_tensor)):
# for i in [25]:
  # print("========entering next input ========")
  # print(new_test_feature_tensor[0:i].shape)


  # ip = new_test_feature_tensor[i]
  # op = new_test_label_tensor[i]

  # print(ip)
  # new_test_feature_tensor_temp = ip.unsqueeze(0)
  # # print(new_test_feature_tensor_temp.shape)
  # new_test_label_tensor_temp = op.unsqueeze(0)
  

  #for loop will come in place
  # methods = ["ig","dl","gs","sa","ixg","occ","dls"]
  # methods = ["ig","sa","dl","gs","ixg","occ","ks","dls"]
  # methods = ["occ","ixg","ig","dl"]
  methods = tp_ip

  for explainers in methods:

    if explainers == "Saliency":
      sa_attr_test = sa.attribute(new_test_feature_tensor, target = new_test_label_tensor)

      sa_attr_test_sum = sa_attr_test.detach().numpy().sum(0)
      sa_attr_test_norm_sum = sa_attr_test_sum / np.linalg.norm(sa_attr_test_sum, ord=1)


      #for simple gradient method
      #top_15_ind = np.argpartition(sa_attr_test_norm_sum, -15)[-15:]
      sa_top_50_ind = np.argsort(sa_attr_test_norm_sum)[:-10]
      
      sa_top_50_ind = np.flip(sa_top_50_ind)

      sa_bool_arr = (sa_attr_test_norm_sum <= 0)
      sa_indices = np.where(sa_bool_arr)[0]

      # print(top_15_ind)
      # print(sa_attr_test_norm_sum[top_15_ind])
      # new_array = sa_attr_test_norm_sum[top_15_ind]
      # # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # # top_15_labels.reverse()

      # print(top_15_labels)
    
    elif explainers == "Integrated Gradients":

      ig_attr_test = ig.attribute(new_test_feature_tensor, target = new_test_label_tensor, n_steps=50)
      ig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)
      ig_attr_test_norm_sum = ig_attr_test_sum / np.linalg.norm(ig_attr_test_sum, ord=1)

      ig_top_50_ind = np.argsort(ig_attr_test_norm_sum)[:-10]
      ig_top_50_ind = np.flip(ig_top_50_ind)

      ig_bool_arr = (ig_attr_test_norm_sum <= 0)
      ig_indices = np.where(ig_bool_arr)[0]

      # new_array = ig_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # top_15_labels
      # print(top_15_labels)

    # methods = ["Integrated Gradients","Saliency","Deep Lift","Gradient Shap","InputXGradient","Occlusion","Kernel Shap"]

    elif explainers == "DeepLift":

      dl_attr_test = dl.attribute(new_test_feature_tensor, target = new_test_label_tensor,)
      dl_attr_test_sum = dl_attr_test.detach().numpy().sum(0)
      # print(dl_attr_test_sum)
      dl_attr_test_norm_sum = dl_attr_test_sum / np.linalg.norm(dl_attr_test_sum, ord=1)
      # print(dl_attr_test_norm_sum)
      # print(type(dl_attr_test_norm_sum))

      dl_top_50_ind = np.argsort(dl_attr_test_norm_sum)[:-10]
      dl_top_50_ind = np.flip(dl_top_50_ind)

      dl_bool_arr = (dl_attr_test_norm_sum <= 0)
      dl_indices = np.where(dl_bool_arr)[0]

      # new_array = dl_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # top_15_labels
      # print(top_15_labels)

    elif explainers == "GradientShap":

      gs_attr_test = gs.attribute(new_test_feature_tensor, target = new_test_label_tensor, baselines = input_tensor[1:50])
      gs_attr_test_sum = gs_attr_test.detach().numpy().sum(0)
      gs_attr_test_norm_sum = gs_attr_test_sum / np.linalg.norm(gs_attr_test_sum, ord=1)
  
      gs_top_50_ind = np.argsort(gs_attr_test_norm_sum)[:-10]
      # print("before flipping")
      # print(top_15_ind)
      # print("after flipping")
      gs_top_50_ind = np.flip(gs_top_50_ind)
      # print(top_15_ind)

      gs_bool_arr = (gs_attr_test_norm_sum <= 0)
      gs_indices = np.where(gs_bool_arr)[0]

      # new_array = gs_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # top_15_labels
      # # print(top_15_labels)
    
    elif explainers == "InputXGradient":
      
      inputXgradient_attr_test = ixg.attribute(new_test_feature_tensor, target = new_test_label_tensor,)
      inputxgradient_attr_test_sum = inputXgradient_attr_test.detach().numpy().sum(0)
      inputxgradient_attr_test_norm_sum = inputxgradient_attr_test_sum / np.linalg.norm(inputxgradient_attr_test_sum, ord=1)



      ixg_top_50_ind = np.argsort(inputxgradient_attr_test_norm_sum)[:-10]
      ixg_top_50_ind = np.flip(ixg_top_50_ind)


      ixg_bool_arr = (inputxgradient_attr_test_norm_sum <= 0)
      ixg_indices = np.where(ixg_bool_arr)[0]

      # new_array = inputxgradient_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # # top_15_labels

    elif explainers == "Occlusion":
      occl_attr_test = occ.attribute(new_test_feature_tensor, sliding_window_shapes=(3,))
      occl_attr_test_sum = occl_attr_test.detach().numpy().sum(0)
      occl_attr_test_norm_sum = occl_attr_test_sum / np.linalg.norm(occl_attr_test_sum, ord=1)


      occ_top_50_ind = np.argsort(occl_attr_test_norm_sum)[:-10]
      occ_top_50_ind = np.flip(occ_top_50_ind)

      occ_bool_arr = (occl_attr_test_norm_sum <= 0)
      occ_indices = np.where(occ_bool_arr)[0]

      # new_array = occl_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # # top_15_labels
    elif explainers == "Shap":

      ks_attr_test = ks.attribute(new_test_feature_tensor, target = new_test_label_tensor)

      ks_attr_test_sum = ks_attr_test.detach().numpy().sum(0)
      ks_attr_test_norm_sum = ks_attr_test_sum / np.linalg.norm(ks_attr_test_sum, ord=1)
    

      ks_top_50_ind = np.argsort(ks_attr_test_norm_sum)[:-10]
      ks_top_50_ind = np.flip(ks_top_50_ind)

      

      ks_bool_arr = (ks_attr_test_norm_sum <= 0)
      ks_indices = np.where(ks_bool_arr)[0]

      # new_array = ks_attr_test_norm_sum[top_15_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # top_15_labels

    elif explainers == "Lime":
      
      # new_test_feature_tensor = new_test_feature_tensor_temp.squeeze()
      # print(new_test_feature_tensor.shape)
      # new_test_label_tensor = new_test_label_tensor_temp.squeeze()

      for i in range(0,len(new_test_feature_tensor)):

        ip = new_test_feature_tensor[i]
        op = new_test_label_tensor[i]

        # print(ip)
        new_test_feature_tensor_temp = ip.unsqueeze(0)
        # print(new_test_feature_tensor_temp.shape)
        new_test_label_tensor_temp = op.unsqueeze(0)


        lime_attr = LimeBase(net,
                       SkLearnLinearModel("linear_model.Ridge"),
                       similarity_func=similarity_kernel,
                       perturb_func=perturb_func,
                       perturb_interpretable_space=False,
                       from_interp_rep_transform=None,
                       to_interp_rep_transform=to_interp_transform)
      
        lime_attr = lime_attr.attribute(new_test_feature_tensor_temp, target = new_test_label_tensor_temp, kernel_width=1.1)
        lime_attr_test_sum = lime_attr.detach().numpy().sum(0)
        lime_attr_test_norm_sum = lime_attr_test_sum / np.linalg.norm(lime_attr_test_sum, ord=1)

        lime_top_50_ind = np.argsort(lime_attr_test_norm_sum)[:-10]
        lime_top_50_ind = np.flip(lime_top_50_ind)

        lime_bool_arr = (lime_attr_test_norm_sum <= 0)
        lime_indices = np.where(lime_bool_arr)[0]

        for idx in lime_top_50_ind:
      #   # print(new_test_feature_tensor_temp[0][idx])
          new_test_feature_tensor_temp[0][idx] = 0

        new_test_feature_tensor[i] = new_test_feature_tensor_temp

    # elif explainers == "dls":
    #   baselines = torch.zeros(new_test_feature_tensor.shape)
    #   dls_attr_test = dls.attribute(new_test_feature_tensor, target = new_test_label_tensor, baselines=baselines)



    #   dls_attr_test_sum = dls_attr_test.detach().numpy().sum(0)
    #   dls_attr_test_norm_sum = dls_attr_test_sum / np.linalg.norm(dls_attr_test_sum, ord=1)

    #   dls_top_50_ind = np.argsort(dls_attr_test_norm_sum)[-50:]
    #   dls_top_50_ind = np.flip(dls_top_50_ind)


      # new_array = dls_attr_test_norm_sum[dls_top_50_ind]
      # top_15_labels = [x_axis_data_labels[i] for i in top_15_ind]
      # top_15_labels.reverse()
      # # top_15_labels



      # methods = ["ig","sa","dl","gs","ixg","occ","ks","dls"]


    # if explainers == "Integrated Gradients":
    #   top_15_ind = ig_indices
    # elif explainers == "Saliency":
    #   top_15_ind = sa_indices
    # elif explainers == "DeepLift":
    #   top_15_ind = dl_indices
    # elif explainers == "GradientShap":
    #   top_15_ind = gs_indices
    # elif explainers == "InputXGradient":
    #   top_15_ind = ixg_indices
    # elif explainers == "Occlusion":
    #   top_15_ind = occ_indices
    # elif explainers == "Shap":
    #   top_15_ind = ks_indices

    if explainers == "Integrated Gradients":
      top_15_ind = ig_top_50_ind
    elif explainers == "Saliency":
      top_15_ind = sa_top_50_ind
    elif explainers == "DeepLift":
      top_15_ind = dl_top_50_ind
    elif explainers == "GradientShap":
      top_15_ind = gs_top_50_ind
    elif explainers == "InputXGradient":
      top_15_ind = ixg_top_50_ind
    elif explainers == "Occlusion":
      top_15_ind = occ_top_50_ind
    elif explainers == "Shap":
      top_15_ind = ks_top_50_ind
    # elif explainers == "Lime":
    #   top_15_ind = lime_top_50_ind
    # elif explainers == "dls":
    #   top_15_ind = dls_top_50_ind


    # for idx in top_15_ind:
    #   # print(new_test_feature_tensor_temp[0][idx])
    #   new_test_feature_tensor_temp[0][idx] = 0

    # new_test_feature_tensor_temp[top_15_ind] = 0
    # new_test_feature_tensor[i] = new_test_feature_tensor_temp
    if explainers != "Lime":
      for i in range(0,len(new_test_feature_tensor)):
        
        for idx in top_15_ind:
        #   # print(new_test_feature_tensor_temp[0][idx])
          new_test_feature_tensor[i][idx] = 0


  return new_test_feature_tensor,new_test_label_tensor

import warnings
warnings.filterwarnings('ignore')

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
from sklearn import metrics

methods = ["Integrated Gradients","Saliency","DeepLift","GradientShap","InputXGradient","Occlusion","Shap","Lime"]
  
tempuu = []

for i in methods:

  s_inn,s_ou = safe_main_func(i)
  m_inn,m_ou = mal_main_func(i)

  c_inn = np.concatenate((s_inn, m_inn))
  c_ou  = np.concatenate((s_ou, m_ou))

  c_inn_tensor = torch.from_numpy(c_inn).type(torch.FloatTensor)
  c_ou_tensor =  torch.from_numpy(c_ou)


  out_probs = net(c_inn_tensor).detach().numpy()
  out_classes = np.argmax(out_probs, axis=1)
  
  fpr, tpr, thresholds = roc_curve(c_ou_tensor,out_classes)
  tempuu.append([fpr,tpr,thresholds,i])

 
  roc_auc = auc(fpr, tpr)




colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k','#800080']

# iterate over each method's data and plot its curve
for i, method_data in enumerate(tempuu):
    fpr, tpr, threshold, method = method_data
    roc_auc = auc(fpr, tpr)
    color = colors[i % len(colors)]
    plt.plot(fpr, tpr, label=method + ' (AUC = %0.2f)' % roc_auc  )
    # plt.plot(fpr, tpr, color=color, label=method)
    
    
# set axis labels and title
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
    
# set legend and show plot

# plt.legend()
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.show()