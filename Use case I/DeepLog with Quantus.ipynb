{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f027a0c7",
   "metadata": {},
   "source": [
    "# Quantative evaluation using Quantus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "978e15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install quantus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b5f97",
   "metadata": {},
   "source": [
    "### Steps to run for XAI methods\n",
    "1. all_metrics_df will store the metrics results for all XAI methods\n",
    "2. Change the export_path which is used to save the result csv \n",
    "3. Add XAI methods which you need to run in interpretation_methods \n",
    "4. finally, run get_all_metrics with the proper params.\n",
    "\n",
    "So, for next XAI methods, just change the method name and rerun the cells.\n",
    "\n",
    "### Following are the metrics which have been calculated:\n",
    "\n",
    "- Model Parameter Randomization\n",
    "- Max-Sensitivity\n",
    "- Relative Output Stability\n",
    "- Monotonicity: Perturb by Blur\n",
    "- Faithfulness\n",
    "- Local Lipschitz Estimate\n",
    "- Sparsity\n",
    "- Complexity\n",
    "\n",
    "### Following are the techniques that have been applied:\n",
    "\n",
    "- Saliency\n",
    "- Input X Gradient\n",
    "- Integrated Gradients\n",
    "- DeepLift\n",
    "- LIME\n",
    "- Kernel SHAP\n",
    "- Occlusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeee788f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6bec0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantus\n",
    "import keras.utils.np_utils as np_utils\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6150746",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5f8452dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_onehot(\n",
       "  (lstm): LSTM(28, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'deepaid')\n",
    "\n",
    "from deepaid.deeplog import *\n",
    "\n",
    "from deepaid.deeplog import LSTM_onehot\n",
    "import torch\n",
    "\n",
    "model = torch.load(\"LSTM_onehot.pth.tar\", map_location=torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d38062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d49d619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(method_name, export_path, model, all_metrics_df, test_features, test_label):\n",
    "    \n",
    "    print(\"==============================================\")\n",
    "    print(\"Running it for: \", method_name)\n",
    "    \n",
    "    print(\"Getting the explaination attribute\")\n",
    "    a_batch_intgrad = quantus.explain(\n",
    "    model, test_features, test_label, method=method_name)\n",
    "    \n",
    "    # Return ModelParameterRandomisation scores for Integrated Gradients.\n",
    "    mpr = quantus.ModelParameterRandomisation(\n",
    "        similarity_func=quantus.similarity_func.correlation_spearman,\n",
    "        return_sample_correlation=True,\n",
    "        aggregate_func=np.mean,\n",
    "        layer_order=\"independent\",\n",
    "        disable_warnings=True,\n",
    "        normalise=True,\n",
    "        abs=True,\n",
    "        display_progressbar=True,\n",
    "    )(\n",
    "        model=model,\n",
    "        x_batch=test_features,\n",
    "        y_batch=test_label,\n",
    "        a_batch=None,\n",
    "        explain_func=quantus.explain,\n",
    "        explain_func_kwargs={\"method\": method_name, \"reduce_axes\": ()},\n",
    "    )\n",
    "\n",
    "    # We will use the same non-default hyperparameters for all metrics.\n",
    "    init_kwargs = dict(\n",
    "        disable_warnings=True,\n",
    "        display_progressbar=True,\n",
    "        abs=True,\n",
    "        normalise=True,\n",
    "        nr_samples=50,\n",
    "        return_nan_when_prediction_changes=True,\n",
    "    )\n",
    "\n",
    "    call_kwargs = dict(\n",
    "        model=model,\n",
    "        x_batch=test_features,\n",
    "        y_batch=test_label,\n",
    "        a_batch=None,\n",
    "        explain_func=quantus.explain,\n",
    "        explain_func_kwargs={\"method\": method_name},\n",
    "        channel_first=True,\n",
    "    )\n",
    "\n",
    "    # Return sparseness scores in an one-liner - by calling the metric instance.\n",
    "    spa = quantus.Sparseness(\n",
    "    )(model=model, \n",
    "       x_batch=test_features,\n",
    "       y_batch=test_label,\n",
    "       a_batch=None,\n",
    "       explain_func=quantus.explain, \n",
    "       explain_func_kwargs={\"method\": method_name})\n",
    "\n",
    "    # Return complexity scores in an one-liner - by calling the metric instance.\n",
    "    com = quantus.Complexity(\n",
    "    )(model=model, \n",
    "       x_batch=test_features,\n",
    "       y_batch=test_label,\n",
    "       a_batch=None,\n",
    "       explain_func=quantus.explain, \n",
    "       explain_func_kwargs={\"method\": method_name})\n",
    "    \n",
    "    # Instantiate metric.\n",
    "    max_sen = quantus.MaxSensitivity(**init_kwargs)\n",
    "    # Evaluate metric.\n",
    "    scores_intgrad_maxs = max_sen(**call_kwargs)\n",
    "\n",
    "    # Instantiate metric\n",
    "    avg_sen = quantus.AvgSensitivity(**init_kwargs)\n",
    "    # Evaluate metric\n",
    "    scores_intgrad_avg_sen = max_sen(**call_kwargs)\n",
    "\n",
    "    # Instantiate metric.\n",
    "    ros = quantus.RelativeOutputStability(**init_kwargs)\n",
    "    # Evaluate metric.\n",
    "    ros_result = ros(**call_kwargs)\n",
    "    ros_result = list(np.log(ros_result))\n",
    "\n",
    "    # Instantiate metric.\n",
    "    lpe = quantus.LocalLipschitzEstimate(**init_kwargs)\n",
    "    # Evaluate metric.\n",
    "    lpe_result = lpe(**call_kwargs)\n",
    "\n",
    "    # Return faithfulness estimate scores in an one-liner - by calling the metric instance.\n",
    "    faith = quantus.FaithfulnessEstimate(\n",
    "        perturb_func=quantus.perturb_func.baseline_replacement_by_blur,\n",
    "        similarity_func=quantus.similarity_func.correlation_pearson,\n",
    "        perturb_baseline=\"black\",\n",
    "    )(model=model, \n",
    "       x_batch=test_features, \n",
    "       y_batch=test_label,\n",
    "       a_batch=a_batch_intgrad)\n",
    "\n",
    "    # Return faithfulness estimate scores in an one-liner - by calling the metric instance.\n",
    "    mono = quantus.MonotonicityCorrelation(\n",
    "        perturb_baseline=\"black\",\n",
    "        perturb_func=quantus.perturb_func.baseline_replacement_by_blur,\n",
    "    )(model=model, \n",
    "       x_batch=test_features, \n",
    "       y_batch=test_label,\n",
    "       a_batch=a_batch_intgrad)\n",
    "    \n",
    "    \n",
    "    # This is for XAI method\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            spa,\n",
    "            com,\n",
    "            faith,\n",
    "            mono,\n",
    "            scores_intgrad_maxs,\n",
    "            lpe_result,\n",
    "            ros_result,\n",
    "            mpr\n",
    "        ],\n",
    "        index=[\n",
    "            \"Sparsity\",\n",
    "            \"Complexity\",\n",
    "            \"Faithfulness\",\n",
    "            \"Monotonicity\",\n",
    "            \"MaxSensitivity\",\n",
    "            \"LocalLipschitzEstimate\",\n",
    "            \"Relative Output Stability\",\n",
    "            \"ModelParameterRadomisation\"\n",
    "        ]\n",
    "    ).aggregate([np.mean, np.std], axis=1)\n",
    "    \n",
    "    print(\"Metric Dataframe:\", df.head())\n",
    "    \n",
    "    # To save metrics for different XAI methods\n",
    "    all_metrics = [method_name] + df[\"mean\"].tolist()\n",
    "    all_metrics_df.loc[len(all_metrics_df.index)] = all_metrics\n",
    "    \n",
    "    \n",
    "    print(\"Exporting results\")\n",
    "    # Saving the results of all XAI method\n",
    "    f_result = os.path.join(export_path, \"all_metrics_quantus_results.csv\")\n",
    "    print(\"All metrics result: \", f_result)\n",
    "    all_metrics_df.to_csv(f_result)\n",
    "\n",
    "\n",
    "    # Saving the result of selected XAI method\n",
    "    result = os.path.join(export_path,method_name + \"_result.csv\")\n",
    "    print(\"Method specific result: \", result)\n",
    "    df.to_csv(result)\n",
    "    \n",
    "    return all_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdde42f",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "431e738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_data = np.load('deepaid/abnormal_data.npy')\n",
    "X = abnormal_data.copy()\n",
    "y, X = X[:,-1], np_utils.to_categorical(X[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5dc86389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define input data batch size\n",
    "test_features = X[1:400]\n",
    "test_label = y[1:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4c837765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Explainable method in Quantus: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['GradientShap',\n",
       " 'IntegratedGradients',\n",
       " 'DeepLift',\n",
       " 'DeepLiftShap',\n",
       " 'InputXGradient',\n",
       " 'Saliency',\n",
       " 'FeatureAblation',\n",
       " 'Deconvolution',\n",
       " 'FeaturePermutation',\n",
       " 'Lime',\n",
       " 'KernelShap',\n",
       " 'LRP',\n",
       " 'Gradient',\n",
       " 'Occlusion',\n",
       " 'LayerGradCam',\n",
       " 'GuidedGradCam',\n",
       " 'LayerConductance',\n",
       " 'LayerActivation',\n",
       " 'InternalInfluence',\n",
       " 'LayerGradientXActivation',\n",
       " 'Control Var. Sobel Filter',\n",
       " 'Control Var. Constant',\n",
       " 'Control Var. Random Uniform']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Available Explainable method in Quantus: \")\n",
    "quantus.helpers.constants.AVAILABLE_XAI_METHODS_CAPTUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b12032c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the export_path and interpretation_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The all_metrics_df will save the results of all the metrics in an dataframe\n",
    "all_metrics_df = pd.DataFrame(columns=['Explainable method Name', 'Sparsity', 'Complexity','Faithfulness', 'Monotonicity', 'MaxSensitivity', 'LocalLipschitzEstimate',\n",
    "'Relative Output Stability', 'ModelParameterRadomisation'])\n",
    "all_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8405e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = \"result/sok_pdf_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7438acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_methods = ['Saliency', 'InputXGradient', 'IntegratedGradients', 'DeepLift', 'KernelShap', 'Occlusion', 'GradientShap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855adcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Running it for:  Saliency\n",
      "Getting the explaination attribute\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df80d3cbe1a459f882fefaa5a2e5801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Sparseness metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Chalasani, Prasad, et al. Concise explanations of neural networks using adversarial training.' International Conference on Machine Learning. PMLR, (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Complexity metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Bhatt, Umang, Adrian Weller, and José MF Moura. 'Evaluating and aggregating feature-based model explanations.' arXiv preprint arXiv:2005.00631 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f6a87d51a84b2692c5fb08fc559049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd5faf0ca0d42b9aa950c4184234293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c1533e381a4a6497dbd4249b3f1b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2955af769254d29b87e3ed528f5ba1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Faithfulness Estimate metric is likely to be sensitive to the choice of baseline value 'perturb_baseline' and similarity function 'similarity_func'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Alvarez-Melis, David, and Tommi S. Jaakkola. 'Towards robust interpretability with self-explaining neural networks.' arXiv preprint arXiv:1806.07538 (2018).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Monotonicity Correlation metric is likely to be sensitive to the choice of baseline value 'perturb_baseline', threshold value 'eps' and number of samples to iterate over 'nr_samples'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Nguyen, An-phi, and María Rodríguez Martínez. 'On quantitative aspects of model interpretability.' arXiv preprint arXiv:2007.07584 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Metric Dataframe:                     mean       std\n",
      "Sparsity        0.304389  0.004113\n",
      "Complexity      4.735277  0.003399\n",
      "Faithfulness    0.017025  0.050162\n",
      "Monotonicity    0.273706  0.083065\n",
      "MaxSensitivity  1.474583  0.191401\n",
      "Exporting results\n",
      "All metrics result:  sok_pdf_result/all_metrics_quantus_results.csv\n",
      "Method specific result:  sok_pdf_result/Saliency_result.csv\n",
      "  Explainable method Name  Sparsity  Complexity  Faithfulness  Monotonicity  \\\n",
      "0                Saliency  0.304389    4.735277      0.017025      0.273706   \n",
      "\n",
      "   MaxSensitivity  LocalLipschitzEstimate  Relative Output Stability  \\\n",
      "0        1.474583                5.175565                  11.378173   \n",
      "\n",
      "   ModelParameterRadomisation  \n",
      "0                    0.617634  \n",
      "==============================================\n",
      "Running it for:  InputXGradient\n",
      "Getting the explaination attribute\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc046c9beda2476c872c8a1e05a0808d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Sparseness metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Chalasani, Prasad, et al. Concise explanations of neural networks using adversarial training.' International Conference on Machine Learning. PMLR, (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Complexity metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Bhatt, Umang, Adrian Weller, and José MF Moura. 'Evaluating and aggregating feature-based model explanations.' arXiv preprint arXiv:2005.00631 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb72b1c98de74fa0b39cd99e15a2fcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5383c6dea67b417eb84923866eb8916a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be819c91a3f14113bdaaf044bff93931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19001d1b86f14095a8813992da7fc103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Faithfulness Estimate metric is likely to be sensitive to the choice of baseline value 'perturb_baseline' and similarity function 'similarity_func'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Alvarez-Melis, David, and Tommi S. Jaakkola. 'Towards robust interpretability with self-explaining neural networks.' arXiv preprint arXiv:1806.07538 (2018).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Monotonicity Correlation metric is likely to be sensitive to the choice of baseline value 'perturb_baseline', threshold value 'eps' and number of samples to iterate over 'nr_samples'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Nguyen, An-phi, and María Rodríguez Martínez. 'On quantitative aspects of model interpretability.' arXiv preprint arXiv:2007.07584 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Metric Dataframe:                     mean       std\n",
      "Sparsity        0.869444  0.054524\n",
      "Complexity      3.042223  0.451541\n",
      "Faithfulness    0.797620  0.108002\n",
      "Monotonicity    0.538618  0.084232\n",
      "MaxSensitivity  0.723199  0.077744\n",
      "Exporting results\n",
      "All metrics result:  sok_pdf_result/all_metrics_quantus_results.csv\n",
      "Method specific result:  sok_pdf_result/InputXGradient_result.csv\n",
      "  Explainable method Name  Sparsity  Complexity  Faithfulness  Monotonicity  \\\n",
      "0                Saliency  0.304389    4.735277      0.017025      0.273706   \n",
      "1          InputXGradient  0.869444    3.042223      0.797620      0.538618   \n",
      "\n",
      "   MaxSensitivity  LocalLipschitzEstimate  Relative Output Stability  \\\n",
      "0        1.474583                5.175565                  11.378173   \n",
      "1        0.723199                2.543471                  19.807406   \n",
      "\n",
      "   ModelParameterRadomisation  \n",
      "0                    0.617634  \n",
      "1                    0.993455  \n",
      "==============================================\n",
      "Running it for:  IntegratedGradients\n",
      "Getting the explaination attribute\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4b9c3d3ff345ad811fb0af7ade6859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Sparseness metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Chalasani, Prasad, et al. Concise explanations of neural networks using adversarial training.' International Conference on Machine Learning. PMLR, (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Complexity metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Bhatt, Umang, Adrian Weller, and José MF Moura. 'Evaluating and aggregating feature-based model explanations.' arXiv preprint arXiv:2005.00631 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cc7fdf338f490abc8a2c7777204659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b68cbfe40a142aeb00b6d49e5c48570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9fbe7679d644979b8613ce6b965d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c9e04279bc44e4aa85c2b9aa393593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Faithfulness Estimate metric is likely to be sensitive to the choice of baseline value 'perturb_baseline' and similarity function 'similarity_func'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Alvarez-Melis, David, and Tommi S. Jaakkola. 'Towards robust interpretability with self-explaining neural networks.' arXiv preprint arXiv:1806.07538 (2018).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Monotonicity Correlation metric is likely to be sensitive to the choice of baseline value 'perturb_baseline', threshold value 'eps' and number of samples to iterate over 'nr_samples'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Nguyen, An-phi, and María Rodríguez Martínez. 'On quantitative aspects of model interpretability.' arXiv preprint arXiv:2007.07584 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Metric Dataframe:                     mean       std\n",
      "Sparsity        0.869279  0.054712\n",
      "Complexity      3.042864  0.452207\n",
      "Faithfulness    0.795696  0.106379\n",
      "Monotonicity    0.538196  0.084148\n",
      "MaxSensitivity  0.654476  0.108863\n",
      "Exporting results\n",
      "All metrics result:  sok_pdf_result/all_metrics_quantus_results.csv\n",
      "Method specific result:  sok_pdf_result/IntegratedGradients_result.csv\n",
      "  Explainable method Name  Sparsity  Complexity  Faithfulness  Monotonicity  \\\n",
      "0                Saliency  0.304389    4.735277      0.017025      0.273706   \n",
      "1          InputXGradient  0.869444    3.042223      0.797620      0.538618   \n",
      "2     IntegratedGradients  0.869279    3.042864      0.795696      0.538196   \n",
      "\n",
      "   MaxSensitivity  LocalLipschitzEstimate  Relative Output Stability  \\\n",
      "0        1.474583                5.175565                  11.378173   \n",
      "1        0.723199                2.543471                  19.807406   \n",
      "2        0.654476                2.163964                  27.255507   \n",
      "\n",
      "   ModelParameterRadomisation  \n",
      "0                    0.617634  \n",
      "1                    0.993455  \n",
      "2                    0.993980  \n",
      "==============================================\n",
      "Running it for:  DeepLift\n",
      "Getting the explaination attribute\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ffe0874ccd4b8cb5adfb03b5629e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Sparseness metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Chalasani, Prasad, et al. Concise explanations of neural networks using adversarial training.' International Conference on Machine Learning. PMLR, (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Complexity metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Bhatt, Umang, Adrian Weller, and José MF Moura. 'Evaluating and aggregating feature-based model explanations.' arXiv preprint arXiv:2005.00631 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce206db414094caeb83e0e4797db61f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed71ccc6c08c4d52883279bca3b07751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742416f20efc4b128c1864ae1457752e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328d83febc4b46cf9df2fdea996ff59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Faithfulness Estimate metric is likely to be sensitive to the choice of baseline value 'perturb_baseline' and similarity function 'similarity_func'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Alvarez-Melis, David, and Tommi S. Jaakkola. 'Towards robust interpretability with self-explaining neural networks.' arXiv preprint arXiv:1806.07538 (2018).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Monotonicity Correlation metric is likely to be sensitive to the choice of baseline value 'perturb_baseline', threshold value 'eps' and number of samples to iterate over 'nr_samples'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Nguyen, An-phi, and María Rodríguez Martínez. 'On quantitative aspects of model interpretability.' arXiv preprint arXiv:2007.07584 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Metric Dataframe:                     mean       std\n",
      "Sparsity        0.869315  0.054681\n",
      "Complexity      3.042721  0.452065\n",
      "Faithfulness    0.796053  0.106853\n",
      "Monotonicity    0.538634  0.084381\n",
      "MaxSensitivity  0.627588  0.106989\n",
      "Exporting results\n",
      "All metrics result:  sok_pdf_result/all_metrics_quantus_results.csv\n",
      "Method specific result:  sok_pdf_result/DeepLift_result.csv\n",
      "  Explainable method Name  Sparsity  Complexity  Faithfulness  Monotonicity  \\\n",
      "0                Saliency  0.304389    4.735277      0.017025      0.273706   \n",
      "1          InputXGradient  0.869444    3.042223      0.797620      0.538618   \n",
      "2     IntegratedGradients  0.869279    3.042864      0.795696      0.538196   \n",
      "3                DeepLift  0.869315    3.042721      0.796053      0.538634   \n",
      "\n",
      "   MaxSensitivity  LocalLipschitzEstimate  Relative Output Stability  \\\n",
      "0        1.474583                5.175565                  11.378173   \n",
      "1        0.723199                2.543471                  19.807406   \n",
      "2        0.654476                2.163964                  27.255507   \n",
      "3        0.627588                2.073017                  27.112575   \n",
      "\n",
      "   ModelParameterRadomisation  \n",
      "0                    0.617634  \n",
      "1                    0.993455  \n",
      "2                    0.993980  \n",
      "3                    0.993950  \n",
      "==============================================\n",
      "Running it for:  KernelShap\n",
      "Getting the explaination attribute\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ab31496577409f89497d9094a61a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Sparseness metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Chalasani, Prasad, et al. Concise explanations of neural networks using adversarial training.' International Conference on Machine Learning. PMLR, (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Complexity metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Bhatt, Umang, Adrian Weller, and José MF Moura. 'Evaluating and aggregating feature-based model explanations.' arXiv preprint arXiv:2005.00631 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc60a543421e43e9bcc4f363c64e1ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5200389467884926a52abcca1af6d6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e902809a03424abbb89a385e5d34e170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19828608fab4cabbf2cce0627a224d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Faithfulness Estimate metric is likely to be sensitive to the choice of baseline value 'perturb_baseline' and similarity function 'similarity_func'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Alvarez-Melis, David, and Tommi S. Jaakkola. 'Towards robust interpretability with self-explaining neural networks.' arXiv preprint arXiv:1806.07538 (2018).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Monotonicity Correlation metric is likely to be sensitive to the choice of baseline value 'perturb_baseline', threshold value 'eps' and number of samples to iterate over 'nr_samples'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Nguyen, An-phi, and María Rodríguez Martínez. 'On quantitative aspects of model interpretability.' arXiv preprint arXiv:2007.07584 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Metric Dataframe:                     mean       std\n",
      "Sparsity        0.473520  0.086950\n",
      "Complexity      4.463303  0.234621\n",
      "Faithfulness    0.119994  0.104203\n",
      "Monotonicity    0.014658  0.080480\n",
      "MaxSensitivity  0.837876  0.285200\n",
      "Exporting results\n",
      "All metrics result:  sok_pdf_result/all_metrics_quantus_results.csv\n",
      "Method specific result:  sok_pdf_result/KernelShap_result.csv\n",
      "  Explainable method Name  Sparsity  Complexity  Faithfulness  Monotonicity  \\\n",
      "0                Saliency  0.304389    4.735277      0.017025      0.273706   \n",
      "1          InputXGradient  0.869444    3.042223      0.797620      0.538618   \n",
      "2     IntegratedGradients  0.869279    3.042864      0.795696      0.538196   \n",
      "3                DeepLift  0.869315    3.042721      0.796053      0.538634   \n",
      "4              KernelShap  0.473520    4.463303      0.119994      0.014658   \n",
      "\n",
      "   MaxSensitivity  LocalLipschitzEstimate  Relative Output Stability  \\\n",
      "0        1.474583                5.175565                  11.378173   \n",
      "1        0.723199                2.543471                  19.807406   \n",
      "2        0.654476                2.163964                  27.255507   \n",
      "3        0.627588                2.073017                  27.112575   \n",
      "4        0.837876                2.738792                  22.323412   \n",
      "\n",
      "   ModelParameterRadomisation  \n",
      "0                    0.617634  \n",
      "1                    0.993455  \n",
      "2                    0.993980  \n",
      "3                    0.993950  \n",
      "4                    0.020554  \n",
      "==============================================\n",
      "Running it for:  Occlusion\n",
      "Getting the explaination attribute\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41342ab7c65448a3ac3c49513cff01ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Sparseness metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Chalasani, Prasad, et al. Concise explanations of neural networks using adversarial training.' International Conference on Machine Learning. PMLR, (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Complexity metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Bhatt, Umang, Adrian Weller, and José MF Moura. 'Evaluating and aggregating feature-based model explanations.' arXiv preprint arXiv:2005.00631 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573dd56759854fe59ab4942ed1958362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fb9f932f024658a5040f4d04eb3a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc42c15858647ee987e868b09a5edc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224c047110a443a897d5cf43d16852e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Faithfulness Estimate metric is likely to be sensitive to the choice of baseline value 'perturb_baseline' and similarity function 'similarity_func'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Alvarez-Melis, David, and Tommi S. Jaakkola. 'Towards robust interpretability with self-explaining neural networks.' arXiv preprint arXiv:1806.07538 (2018).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Monotonicity Correlation metric is likely to be sensitive to the choice of baseline value 'perturb_baseline', threshold value 'eps' and number of samples to iterate over 'nr_samples'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Nguyen, An-phi, and María Rodríguez Martínez. 'On quantitative aspects of model interpretability.' arXiv preprint arXiv:2007.07584 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Metric Dataframe:                     mean       std\n",
      "Sparsity        0.892324  0.061630\n",
      "Complexity      2.773011  0.614408\n",
      "Faithfulness    0.917612  0.102745\n",
      "Monotonicity    0.549331  0.071561\n",
      "MaxSensitivity  0.605228  0.110777\n",
      "Exporting results\n",
      "All metrics result:  sok_pdf_result/all_metrics_quantus_results.csv\n",
      "Method specific result:  sok_pdf_result/Occlusion_result.csv\n",
      "  Explainable method Name  Sparsity  Complexity  Faithfulness  Monotonicity  \\\n",
      "0                Saliency  0.304389    4.735277      0.017025      0.273706   \n",
      "1          InputXGradient  0.869444    3.042223      0.797620      0.538618   \n",
      "2     IntegratedGradients  0.869279    3.042864      0.795696      0.538196   \n",
      "3                DeepLift  0.869315    3.042721      0.796053      0.538634   \n",
      "4              KernelShap  0.473520    4.463303      0.119994      0.014658   \n",
      "5               Occlusion  0.892324    2.773011      0.917612      0.549331   \n",
      "\n",
      "   MaxSensitivity  LocalLipschitzEstimate  Relative Output Stability  \\\n",
      "0        1.474583                5.175565                  11.378173   \n",
      "1        0.723199                2.543471                  19.807406   \n",
      "2        0.654476                2.163964                  27.255507   \n",
      "3        0.627588                2.073017                  27.112575   \n",
      "4        0.837876                2.738792                  22.323412   \n",
      "5        0.605228                2.068668                  15.448892   \n",
      "\n",
      "   ModelParameterRadomisation  \n",
      "0                    0.617634  \n",
      "1                    0.993455  \n",
      "2                    0.993980  \n",
      "3                    0.993950  \n",
      "4                    0.020554  \n",
      "5                    0.929352  \n",
      "==============================================\n",
      "Running it for:  GradientShap\n",
      "Getting the explaination attribute\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4671234783f94f2a85c9fcc7a7c83f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Sparseness metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Chalasani, Prasad, et al. Concise explanations of neural networks using adversarial training.' International Conference on Machine Learning. PMLR, (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Complexity metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Bhatt, Umang, Adrian Weller, and José MF Moura. 'Evaluating and aggregating feature-based model explanations.' arXiv preprint arXiv:2005.00631 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eeb5f002294a3ab1c631f0fbf687e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2729837ca0384a2fb0cc63b8f2cbeac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7351cbe48ddd4d358603e61d4ae2ac89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61af0e3be628499dbe06e6db77cd8d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Faithfulness Estimate metric is likely to be sensitive to the choice of baseline value 'perturb_baseline' and similarity function 'similarity_func'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Alvarez-Melis, David, and Tommi S. Jaakkola. 'Towards robust interpretability with self-explaining neural networks.' arXiv preprint arXiv:1806.07538 (2018).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Monotonicity Correlation metric is likely to be sensitive to the choice of baseline value 'perturb_baseline', threshold value 'eps' and number of samples to iterate over 'nr_samples'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Nguyen, An-phi, and María Rodríguez Martínez. 'On quantitative aspects of model interpretability.' arXiv preprint arXiv:2007.07584 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for method_name in interpretation_methods:\n",
    "    \n",
    "    all_metrics_df = get_all_metrics(method_name, export_path, model, all_metrics_df, test_features, test_label)\n",
    "    \n",
    "    print(all_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9814be7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667d510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88893fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ec89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac05a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93adab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d568576e",
   "metadata": {},
   "source": [
    "## Experimental Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "21f3eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = \"Occlusion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "60a36324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399, 10, 28)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bedcf81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only below cells needs to  rerun for different XAI methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "57ef2c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9058d3b195948788e2600e0aa35880d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Sparseness metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Chalasani, Prasad, et al. Concise explanations of neural networks using adversarial training.' International Conference on Machine Learning. PMLR, (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Complexity metric is likely to be sensitive to the choice of normalising 'normalise' (and 'normalise_func') and if taking absolute values of attributions 'abs'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Bhatt, Umang, Adrian Weller, and José MF Moura. 'Evaluating and aggregating feature-based model explanations.' arXiv preprint arXiv:2005.00631 (2020).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de16dc43afe414c9e8fc6513d673f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajayshewale/miniforge3/envs/explainable_env/lib/python3.10/site-packages/quantus/helpers/warn.py:263: UserWarning: The settings for perturbing input e.g., 'perturb_func' didn't cause change in input. Reconsider the parameter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936a03ec2940452bbcf552eedab3e3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b135a88a85a4341bae58de6cf847a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ba6a5aa31046fd8e8099150c386348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnings and information:\n",
      " (1) The Faithfulness Estimate metric is likely to be sensitive to the choice of baseline value 'perturb_baseline' and similarity function 'similarity_func'.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Alvarez-Melis, David, and Tommi S. Jaakkola. 'Towards robust interpretability with self-explaining neural networks.' arXiv preprint arXiv:1806.07538 (2018).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n",
      "Warnings and information:\n",
      " (1) The Monotonicity metric is likely to be sensitive to the choice of baseline value 'perturb_baseline', also, the monotonicity constraint between your given model and explanation method should be assessed.  \n",
      " (2) If attributions are normalised or their absolute values are taken it may destroy or skew information in the explanation and as a result, affect the overall evaluation outcome.\n",
      " (3) Make sure to validate the choices for hyperparameters of the metric (by calling .get_params of the metric instance).\n",
      " (4) For further information, see original publication: Arya, Vijay, et al. 'One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques.' arXiv preprint arXiv:1909.03012 (2019).\n",
      " (5) To disable these warnings set 'disable_warnings' = True when initialising the metric.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_batch_intgrad = quantus.explain(\n",
    "    model, test_features, test_label, method=method_name\n",
    ")\n",
    "\n",
    "# Return ModelParameterRandomisation scores for Integrated Gradients.\n",
    "mpr = quantus.ModelParameterRandomisation(\n",
    "    similarity_func=quantus.similarity_func.correlation_spearman,\n",
    "    return_sample_correlation=True,\n",
    "    aggregate_func=np.mean,\n",
    "    layer_order=\"independent\",\n",
    "    disable_warnings=True,\n",
    "    normalise=True,\n",
    "    abs=True,\n",
    "    display_progressbar=True,\n",
    ")(\n",
    "    model=model,\n",
    "    x_batch=test_features,\n",
    "    y_batch=test_label,\n",
    "    a_batch=None,\n",
    "    explain_func=quantus.explain,\n",
    "    explain_func_kwargs={\"method\": method_name, \"reduce_axes\": ()},\n",
    ")\n",
    "\n",
    "# We will use the same non-default hyperparameters for all metrics.\n",
    "init_kwargs = dict(\n",
    "    disable_warnings=True,\n",
    "    display_progressbar=True,\n",
    "    abs=True,\n",
    "    normalise=True,\n",
    "    nr_samples=50,\n",
    "    return_nan_when_prediction_changes=True,\n",
    ")\n",
    "\n",
    "call_kwargs = dict(\n",
    "    model=model,\n",
    "    x_batch=test_features,\n",
    "    y_batch=test_label,\n",
    "    a_batch=None,\n",
    "    explain_func=quantus.explain,\n",
    "    explain_func_kwargs={\"method\": method_name},\n",
    "    channel_first=True,\n",
    ")\n",
    "\n",
    "# Return sparseness scores in an one-liner - by calling the metric instance.\n",
    "spa = quantus.Sparseness(\n",
    ")(model=model, \n",
    "   x_batch=test_features,\n",
    "   y_batch=test_label,\n",
    "   a_batch=None,\n",
    "   explain_func=quantus.explain, \n",
    "   explain_func_kwargs={\"method\": method_name})\n",
    "\n",
    "# Return complexity scores in an one-liner - by calling the metric instance.\n",
    "com = quantus.Complexity(\n",
    ")(model=model, \n",
    "   x_batch=test_features,\n",
    "   y_batch=test_label,\n",
    "   a_batch=None,\n",
    "   explain_func=quantus.explain, \n",
    "   explain_func_kwargs={\"method\": method_name},\n",
    "   device=device)\n",
    "\n",
    "# Instantiate metric.\n",
    "max_sen = quantus.MaxSensitivity(**init_kwargs)\n",
    "# Evaluate metric.\n",
    "scores_intgrad_maxs = max_sen(**call_kwargs)\n",
    "\n",
    "# Instantiate metric\n",
    "avg_sen = quantus.AvgSensitivity(**init_kwargs)\n",
    "# Evaluate metric\n",
    "scores_intgrad_avg_sen = max_sen(**call_kwargs)\n",
    "\n",
    "# Instantiate metric.\n",
    "ros = quantus.RelativeOutputStability(**init_kwargs)\n",
    "# Evaluate metric.\n",
    "ros_result = ros(**call_kwargs)\n",
    "ros_result = list(np.log(ros_result))\n",
    "\n",
    "# Instantiate metric.\n",
    "lpe = quantus.LocalLipschitzEstimate(**init_kwargs)\n",
    "# Evaluate metric.\n",
    "lpe_result = lpe(**call_kwargs)\n",
    "\n",
    "# Return faithfulness estimate scores in an one-liner - by calling the metric instance.\n",
    "faith = quantus.FaithfulnessEstimate(\n",
    "    perturb_func=quantus.perturb_func.baseline_replacement_by_blur,\n",
    "    similarity_func=quantus.similarity_func.correlation_pearson,\n",
    "    perturb_baseline=\"black\",\n",
    ")(model=model, \n",
    "   x_batch=test_features, \n",
    "   y_batch=test_label,\n",
    "   a_batch=a_batch_intgrad)\n",
    "\n",
    "# Return faithfulness estimate scores in an one-liner - by calling the metric instance.\n",
    "mono = quantus.Monotonicity(\n",
    "    perturb_baseline=\"black\",\n",
    "    perturb_func=quantus.perturb_func.baseline_replacement_by_blur,\n",
    ")(model=model, \n",
    "   x_batch=test_features, \n",
    "   y_batch=test_label,\n",
    "   a_batch=a_batch_intgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b70535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "42282d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for XAI method\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        spa,\n",
    "        com,\n",
    "        faith,\n",
    "        mono,\n",
    "        scores_intgrad_maxs,\n",
    "        lpe_result,\n",
    "        ros_result,\n",
    "        mpr\n",
    "    ],\n",
    "    index=[\n",
    "        \"Sparsity\",\n",
    "        \"Complexity\",\n",
    "        \"Faithfulness\",\n",
    "        \"Monotonicity\",\n",
    "        \"MaxSensitivity\",\n",
    "        \"LocalLipschitzEstimate\",\n",
    "        \"Relative Output Stability\",\n",
    "        \"ModelParameterRadomisation\"\n",
    "    ]\n",
    ").aggregate([np.mean, np.std], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b1e2b17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sparsity</th>\n",
       "      <td>0.596379</td>\n",
       "      <td>0.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Complexity</th>\n",
       "      <td>2.629109</td>\n",
       "      <td>0.293026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Faithfulness</th>\n",
       "      <td>0.430808</td>\n",
       "      <td>0.169039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monotonicity</th>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxSensitivity</th>\n",
       "      <td>0.618571</td>\n",
       "      <td>0.091133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LocalLipschitzEstimate</th>\n",
       "      <td>1.262049</td>\n",
       "      <td>0.213832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relative Output Stability</th>\n",
       "      <td>11.805531</td>\n",
       "      <td>2.287362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ModelParameterRadomisation</th>\n",
       "      <td>0.984805</td>\n",
       "      <td>0.004984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 mean       std\n",
       "Sparsity                     0.596379  0.113100\n",
       "Complexity                   2.629109  0.293026\n",
       "Faithfulness                 0.430808  0.169039\n",
       "Monotonicity                 0.002506  0.000000\n",
       "MaxSensitivity               0.618571  0.091133\n",
       "LocalLipschitzEstimate       1.262049  0.213832\n",
       "Relative Output Stability   11.805531  2.287362\n",
       "ModelParameterRadomisation   0.984805  0.004984"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "34d122dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save metrics for different XAI methods\n",
    "all_metrics = [method_name] + df[\"mean\"].tolist()\n",
    "all_metrics_df.loc[len(all_metrics_df.index)] = all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "abaf3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results of all XAI method\n",
    "f_result = \"all_metrics_quantus_results.csv\"\n",
    "all_metrics_df.to_csv(f_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a0d12174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the result of selected XAI method\n",
    "result = method_name + \"_result.csv\"\n",
    "df.to_csv(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "be99c6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Occlusion',\n",
       " 0.5963786203691944,\n",
       " 2.629108637714053,\n",
       " 0.43080817331157095,\n",
       " 0.002506265664160401,\n",
       " 0.6185713574594381,\n",
       " 1.2620486510130937,\n",
       " 11.805531355073663,\n",
       " 0.9848051586175459]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25143904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
